{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(train_data_path, test_data_path, delimiter=\"|\"):\n",
    "    # Load the training and test data\n",
    "    train_data = pd.read_csv(train_data_path, delimiter=delimiter)\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    test_data = test_data.drop(columns=[\"ID\", \"scaling_factor\"])\n",
    "\n",
    "    # Convert time column to datetime\n",
    "    train_data['time'] = pd.to_datetime(train_data['time'])\n",
    "\n",
    "    # Sort the training data by vesselId and time to ensure proper alignment for shifting\n",
    "    train_data_sorted = train_data.sort_values(by=[\"vesselId\", \"time\"], ascending=True).copy()\n",
    "\n",
    "    # Shift latitude and longitude for each vessel to create \"previous_lat\" and \"previous_lon\"\n",
    "    train_data_sorted['previous_lat'] = train_data_sorted.groupby('vesselId')['latitude'].shift(1)\n",
    "    train_data_sorted['previous_lon'] = train_data_sorted.groupby('vesselId')['longitude'].shift(1)\n",
    "\n",
    "    # Get the last entry per vessel in the training data\n",
    "    last_entries = train_data_sorted.groupby('vesselId').last().reset_index()\n",
    "    \n",
    "    # Merge the last row of the training data with the test data based on vesselId\n",
    "    test_data_merged = test_data.merge(last_entries, on='vesselId', how='left')\n",
    "    test_data_merged = test_data_merged.drop(columns=['time_y']).rename(columns={'time_x': 'time'})\n",
    "\n",
    "    return train_data_sorted, test_data_merged\n",
    "\n",
    "# Usage\n",
    "from settings import *\n",
    "\n",
    "train_data_path = AIS_TRAIN # '../data/ais_train.csv'\n",
    "test_data_path = AIS_TEST # '../data/ais_test.csv'\n",
    "\n",
    "processed_train_data, processed_test_data = process_data(train_data_path, test_data_path)\n",
    "\n",
    "# To save the processed test data\n",
    "processed_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_test = processed_test_data\n",
    "X_test = X_test.drop(columns=[\"longitude\", \"latitude\", \"etaRaw\"])\n",
    "\n",
    "# Split training data into features and target variables\n",
    "y_train = processed_train_data[[\"latitude\", \"longitude\"]].copy()\n",
    "X_train = processed_train_data.copy()\n",
    "X_train = X_train.drop(columns=[\"longitude\", \"latitude\", \"etaRaw\"])\n",
    "\n",
    "\n",
    "# Basic preprocessing (converting time and etaRaw to numerical features)\n",
    "X_train['time'] = pd.to_datetime(X_train['time'], errors='coerce')\n",
    "\n",
    "X_train['year'] = X_train['time'].dt.year\n",
    "X_train['month'] = X_train['time'].dt.month\n",
    "X_train['day'] = X_train['time'].dt.day\n",
    "X_train['hour'] = X_train['time'].dt.hour\n",
    "\n",
    "# Drop the original time and etaRaw columns after extracting useful features\n",
    "X_train = X_train.drop(columns=['time'])\n",
    "\n",
    "# Apply the same preprocessing to the test set\n",
    "X_test['time'] = pd.to_datetime(X_test['time'], errors='coerce')\n",
    "\n",
    "X_test['year'] = X_test['time'].dt.year\n",
    "X_test['month'] = X_test['time'].dt.month\n",
    "X_test['day'] = X_test['time'].dt.day\n",
    "X_test['hour'] = X_test['time'].dt.hour\n",
    "\n",
    "X_test = X_test.drop(columns=['time'])\n",
    "\n",
    "# Encode categorical variables 'vesselId' and 'portId' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "X_train['vesselId'] = label_encoder.fit_transform(X_train['vesselId'].astype(str))\n",
    "\n",
    "#Badpractice, should be label_encoder.transform() but this gives an error so instead i use fit_transform\n",
    "X_test['vesselId'] = label_encoder.fit_transform(X_test['vesselId'].astype(str))\n",
    "\n",
    "X_train['portId'] = label_encoder.fit_transform(X_train['portId'].astype(str))\n",
    "\n",
    "#Badpractice, should be label_encoder.transform() but this gives an error so instead i use fit_transform\n",
    "X_test['portId'] = label_encoder.fit_transform(X_test['portId'].astype(str))\n",
    "\n",
    "X_train = X_train[['cog', 'sog', 'rot', 'heading', 'navstat', 'vesselId', 'portId', 'previous_lat', 'previous_lon']]\n",
    "X_test = X_test[['cog', 'sog', 'rot', 'heading', 'navstat', 'vesselId', 'portId', 'previous_lat', 'previous_lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # 'n_estimators': 5000,\n",
    "    'gamma': 0.5,\n",
    "    'subsample': 0.6,\n",
    "    'n_estimators': 5000,\n",
    "    'min_child_weight':  15,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'max_depth': 4,\n",
    "    'eta':  0.005,\n",
    "    'refresh_leaf': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost Regressor\n",
    "xgb_reg = xgb.XGBRegressor()\n",
    "\n",
    "# MultiOutputRegressor to handle the two outputs (latitude and longitude)\n",
    "multi_regressor = MultiOutputRegressor(xgb_reg)\n",
    "\n",
    "# Train the model\n",
    "multi_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions over X_test\n",
    "y_pred_default = multi_regressor.predict(X_test)\n",
    "\n",
    "# Convert the NumPy array to a pandas DataFrame\n",
    "y_pred_default_df = pd.DataFrame(y_pred_default, columns=['latitude_predicted', 'longitude_predicted'])\n",
    "\n",
    "# Add the ID column, which starts from 0 and increments by 1 for each row\n",
    "y_pred_default_df['ID'] = range(len(y_pred_default_df))\n",
    "\n",
    "# Reorder the columns to ensure 'id' is the first column (if required by the submission format)\n",
    "y_pred_default_df = y_pred_default_df[['ID', 'longitude_predicted', 'latitude_predicted']]\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "from utils import make_file_name\n",
    "from settings import * \n",
    "\n",
    "output_path = SUBMISSION_FODLER.joinpath(make_file_name() + '.csv')\n",
    "y_pred_default_df.to_csv(output_path, index=False)\n",
    "\n",
    "y_pred_default_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitask3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
