{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import *\n",
    "\n",
    "from src.data.preprocessing import preprocess, features_input\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from typing import Dict, Union, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ais_train = pd.read_csv(AIS_TRAIN, sep='|')\n",
    "ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "\n",
    "ais_test = pd.read_csv(AIS_TEST, sep=\",\")\n",
    "ais_test['time'] = pd.to_datetime(ais_test['time']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 48\n",
    "do_preprocess = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_forecast(seq, model, steps, sequence_length):\n",
    "    predicted = []\n",
    "    current_sequence = seq[:sequence_length].reshape(1,sequence_length,7)\n",
    "    # current_sequence = last_known[-sequence_length:]\n",
    "    for k in range(steps):\n",
    "        # next_pred = model.predict(current_sequence.reshape(1, sequence_length, -1))[0]\n",
    "        X_test = torch.Tensor(current_sequence).to(DEVICE)\n",
    "        y_pred = model(X_test)[0,0,:]\n",
    "\n",
    "        next_pred = y_pred.view(6).cpu().numpy()\n",
    "        predicted.append(next_pred)\n",
    "\n",
    "        seq[k+1,1:] = next_pred\n",
    "        # Update current_sequence by appending next prediction\n",
    "        current_sequence = seq[k+1,].view(1,sequence_length,7)\n",
    "    \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(seq_len = 48, do_preprocess = True):\n",
    "    if do_preprocess:\n",
    "        ais_train = pd.read_csv(AIS_TRAIN, sep='|')\n",
    "        ais_train['time'] = pd.to_datetime(ais_train['time'])\n",
    "\n",
    "        ais_test = pd.read_csv(AIS_TEST, sep=\",\")\n",
    "        ais_test['time'] = pd.to_datetime(ais_test['time']) \n",
    "\n",
    "        X_train, X_val, y_train, y_val, test_set, scaler = preprocess(\n",
    "            ais_train, \n",
    "            ais_test,\n",
    "            seq_type=\"n_in_1_out\",\n",
    "            seq_len=seq_len,\n",
    "            seq_len_out=1,\n",
    "            verbose=True,\n",
    "            to_torch=True\n",
    "        )\n",
    "\n",
    "        X_train = torch.Tensor(X_train)\n",
    "        y_train = torch.Tensor(y_train)\n",
    "\n",
    "        X_val = torch.Tensor(X_val)\n",
    "        y_val = torch.Tensor(y_val)\n",
    "\n",
    "        torch.save(X_train, LAST_PREPROCESS_FOLDER.joinpath(\"X_train.pt\"))\n",
    "        torch.save(y_train, LAST_PREPROCESS_FOLDER.joinpath(\"y_train.pt\"))\n",
    "        torch.save(X_val, LAST_PREPROCESS_FOLDER.joinpath(\"X_val.pt\"))\n",
    "        torch.save(y_val, LAST_PREPROCESS_FOLDER.joinpath(\"y_val.pt\"))\n",
    "\n",
    "        joblib.dump(scaler, LAST_PREPROCESS_FOLDER.joinpath(\"scaler\")) \n",
    "        test_set.to_csv(LAST_PREPROCESS_FOLDER.joinpath(\"test_set.csv\"))\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            \n",
    "            X_train = torch.load(LAST_PREPROCESS_FOLDER.joinpath(\"X_train.pt\"), weights_only=True)\n",
    "            y_train = torch.load(LAST_PREPROCESS_FOLDER.joinpath(\"y_train.pt\"), weights_only=True)\n",
    "            X_val = torch.load(LAST_PREPROCESS_FOLDER.joinpath(\"X_val.pt\"), weights_only=True)\n",
    "            y_val = torch.load(LAST_PREPROCESS_FOLDER.joinpath(\"y_val.pt\"), weights_only=True)\n",
    "\n",
    "            scaler = joblib.load(LAST_PREPROCESS_FOLDER.joinpath(\"scaler\")) \n",
    "            test_set = pd.read_csv(LAST_PREPROCESS_FOLDER.joinpath(\"test_set.csv\"))\n",
    "\n",
    "        except:\n",
    "            print(f\"ERROR: File missing in {str(LAST_PREPROCESS_FOLDER)}. Now run preprocessing...\")\n",
    "            return pipeline(seq_len=seq_len, do_preprocess=True)\n",
    "        \n",
    "    return X_train, X_val, y_train, y_val, test_set, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, test_set, scaler = pipeline(seq_len, do_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_ffn = 126\n",
    "d_model = 32\n",
    "activation_dec: Union[str | Callable[[torch.Tensor], torch.Tensor]] = nn.SiLU()\n",
    "\n",
    "transformer_decoder_params = {\n",
    "    \"d_model\": d_model,\n",
    "    \"nhead\": 8,\n",
    "    # \"num_encoder_layers\": 6,\n",
    "    # \"num_decoder_layers\": 2,\n",
    "    \"dim_feedforward\": dim_ffn,\n",
    "    \"dropout\": 0.1,\n",
    "    \"activation\": activation_dec,\n",
    "    \"layer_norm_eps\": 0.00001,\n",
    "    \"batch_first\": True,\n",
    "    \"norm_first\": False,\n",
    "    # \"bias\": True,\n",
    "    \"device\": DEVICE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            decoder_params: Dict[int,Union[int, float, bool]] = transformer_decoder_params, \n",
    "            num_features: int = 7, \n",
    "            num_outputs: int = 6, \n",
    "            num_layers: int = 1,\n",
    "            act_out: nn.Module | None = None\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.emb_layer = nn.Linear(num_features, d_model)\n",
    "        dec_layer = nn.TransformerDecoderLayer(**decoder_params)\n",
    "        self.model = nn.TransformerDecoder(dec_layer, num_layers=num_layers)\n",
    "        self.ffn = nn.Linear(d_model, num_outputs)\n",
    "        self.act_out = act_out # nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb_layer(x)\n",
    "        out = self.model(emb, emb)\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        if self.act_out:\n",
    "            return self.act_out(self.ffn(out))\n",
    "        return self.ffn(out)\n",
    "\n",
    "\n",
    "model = DecoderModel(act_out=nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train.trainer import Trainer\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    loss=nn.MSELoss(),\n",
    "    optimizer=torch.optim.Adam(params=model.parameters()),\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(X_train).to(DEVICE)\n",
    "y_train = torch.Tensor(y_train).to(DEVICE)\n",
    "\n",
    "X_val = torch.Tensor(X_val).to(DEVICE)\n",
    "y_val = torch.Tensor(y_val).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    # X_val=X_val,\n",
    "    # y_val=y_val,\n",
    "    epochs=1,\n",
    "    eval_on_test=True,\n",
    "    k_folds=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(X_val)\n",
    "\n",
    "score = trainer.metric(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    print(\"Score on validation set (rmse):\", np.sqrt(score))\n",
    "except:\n",
    "    print(\"Score on validation set:\", np.sqrt(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION STEP\n",
    "\n",
    "grouped_test = test_set.groupby(\"vesselId\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "sequence_length = seq_len\n",
    "\n",
    "for vessel_id, group in grouped_test:\n",
    "    forecast_steps = len(group['time'].values) - 1\n",
    "\n",
    "    last_known_features = group[features_input].values\n",
    "\n",
    "    # last_known_features = scaler.transform(group[scaled_features].values)\n",
    "    future_preds = iterative_forecast(last_known_features, trainer, forecast_steps, sequence_length)\n",
    "    \n",
    "    # Store the predictions\n",
    "\n",
    "    df_pred = pd.DataFrame(scaler.inverse_transform(future_preds), columns=features_input)\n",
    "    df_pred['time'] = group['time'].iloc[1:].values\n",
    "    df_pred[\"vesselId\"] = vessel_id\n",
    "    predictions.append(df_pred)\n",
    "\n",
    "df_preds = pd.concat(predictions, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBMIT RESULT\n",
    "\n",
    "res = pd.merge(ais_test, df_preds[[\"vesselId\",\"time\", \"latitude\", \"longitude\"]],on=[\"time\", \"vesselId\"], how=\"left\")\n",
    "res[\"longitude_predicted\"] = res[\"longitude\"]\n",
    "res[\"latitude_predicted\"] = res[\"latitude\"]\n",
    "# res[\"id\"] = res[\"ID\"]\n",
    "res = res.drop(columns=[\"longitude\", \"latitude\"])\n",
    "\n",
    "def make_file_name() -> str:\n",
    "    file_name = str(uuid.uuid4()) + \".csv\"\n",
    "    print(f\"Submission file name is: {file_name}\")\n",
    "    return file_name\n",
    "\n",
    "def submit(forecast: pd.DataFrame, file_name: str = None) -> None:\n",
    "    sample_submission = pd.read_csv(AIS_SAMPLE_SUBMISSION)\n",
    "    file_name = file_name if file_name else make_file_name()\n",
    "\n",
    "    repertory = SUBMISSION_FODLER.joinpath(file_name)\n",
    "    sample_submission = sample_submission[['ID']].merge(forecast[[\"ID\",\"longitude_predicted\",\"latitude_predicted\"]], on='ID', how='left')\n",
    "    try:\n",
    "        sample_submission.to_csv(repertory, index=False)\n",
    "    except:\n",
    "        print(\"Error register file\")\n",
    "        submit(forecast)\n",
    "\n",
    "submit(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitask3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
