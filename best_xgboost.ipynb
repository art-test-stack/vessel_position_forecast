{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8ef4f-78de-417a-ae79-9d08377cee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023318e-40e7-4d7c-8e50-2bd4ad53e7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(train_data_path, test_data_path, delimiter=\"|\"):\n",
    "    train_data = pd.read_csv(train_data_path, delimiter=delimiter)\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    test_data = test_data.drop(columns=[\"ID\", \"scaling_factor\"])\n",
    "\n",
    "    train_data['time'] = pd.to_datetime(train_data['time'])\n",
    "\n",
    "    train_data_sorted = train_data.sort_values(by=[\"vesselId\", \"time\"], ascending=True).copy()\n",
    "\n",
    "    train_data_sorted['previous_lat'] = train_data_sorted.groupby('vesselId')['latitude'].shift(1)\n",
    "    train_data_sorted['previous_lon'] = train_data_sorted.groupby('vesselId')['longitude'].shift(1)\n",
    "\n",
    "    last_entries = train_data_sorted.groupby('vesselId').last().reset_index()\n",
    "    \n",
    "    test_data_merged = test_data.merge(last_entries, on='vesselId', how='left')\n",
    "    test_data_merged = test_data_merged.drop(columns=['time_y']).rename(columns={'time_x': 'time'})\n",
    "\n",
    "    return train_data_sorted, test_data_merged\n",
    "\n",
    "train_data_path = AIS_TRAIN \n",
    "test_data_path = AIS_TEST\n",
    "\n",
    "processed_train_data, processed_test_data = process_data(train_data_path, test_data_path)\n",
    "\n",
    "display(processed_test_data.head())\n",
    "\n",
    "# Load data\n",
    "X_test = processed_test_data\n",
    "X_test = X_test.drop(columns=[\"longitude\", \"latitude\", \"etaRaw\"])\n",
    "\n",
    "# Prepare features and target variables\n",
    "y_train = processed_train_data[[\"latitude\", \"longitude\"]].copy()\n",
    "X_train = processed_train_data.copy()\n",
    "\n",
    "X_train = X_train.drop(columns=['time'])\n",
    "\n",
    "# Apply the same preprocessing to the test set\n",
    "X_test = processed_test_data.copy()\n",
    "X_test = X_test.drop(columns=['time'])\n",
    "\n",
    "# Encode categorical variables 'vesselId' and 'portId' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "X_train['vesselId'] = label_encoder.fit_transform(X_train['vesselId'].astype(str))\n",
    "X_test['vesselId'] = label_encoder.transform(X_test['vesselId'].astype(str))  # Use transform to avoid data leakage\n",
    "\n",
    "X_train['portId'] = label_encoder.fit_transform(X_train['portId'].astype(str))\n",
    "X_test['portId'] = label_encoder.transform(X_test['portId'].astype(str)) # Use transform to avoid data leakage\n",
    "\n",
    "# Select relevant features for training and testing\n",
    "X_train = X_train[['cog', 'sog', 'previous_lat', 'previous_lon', 'portId', \"heading\",\"vesselId\"]]\n",
    "X_test = X_test[['cog', 'sog', 'previous_lat', 'previous_lon', 'portId', \"heading\",\"vesselId\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd3e38-f65d-4429-bf42-4f3e36fa7743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'colsample_bytree': 0.8, \n",
    "  'learning_rate': 0.1, \n",
    "  'max_depth': 6,\n",
    "  'min_child_weight': 1,\n",
    "  'n_estimators': 100,\n",
    "  'subsample': 1.0\n",
    "}\n",
    "\n",
    "xreg = xgb.XGBRegressor()\n",
    "\n",
    "multi_regressor = MultiOutputRegressor(xreg)\n",
    "multi_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08464469-b61a-43b3-8d29-fb4bfb4e19db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'estimator__n_estimators': [100,200,300],  \n",
    "    'estimator__max_depth': [4, 5,6], \n",
    "    'estimator__learning_rate': [0.01, 0.1,0.005], \n",
    "    'estimator__subsample': [0.9,1.0],\n",
    "    'estimator__min_child_weight': [1,2], \n",
    "    'estimator__colsample_bytree': [0.7,0.8] \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=multi_regressor,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    verbose=10,\n",
    "    n_jobs=-1 \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74073a6-264e-4d93-a6ae-2f365b288ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions over X_test with result of grid schearch \n",
    "y_pred_default = grid_search.predict(X_test)\n",
    "\n",
    "# Make predictions over X_test with fix params \n",
    "#y_pred_default = multi_regressor.predict(X_test)\n",
    "\n",
    "# Convert the NumPy array to a pandas DataFrame\n",
    "y_pred_default_df = pd.DataFrame(y_pred_default, columns=['latitude_predicted', 'longitude_predicted'])\n",
    "\n",
    "# Add the ID column, which starts from 0 and increments by 1 for each row\n",
    "y_pred_default_df['ID'] = range(len(y_pred_default_df))\n",
    "\n",
    "# Reorder the columns to ensure 'id' is the first column (if required by the submission format)\n",
    "y_pred_default_df = y_pred_default_df[['ID', 'longitude_predicted', 'latitude_predicted']]\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "from utils import make_file_name\n",
    "from settings import * \n",
    "\n",
    "output_path = SUBMISSION_FODLER.joinpath(make_file_name() + '.csv')\n",
    "y_pred_default_df.to_csv(output_path, index=False)\n",
    "\n",
    "y_pred_default_df.head()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu118.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu118:m125"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
