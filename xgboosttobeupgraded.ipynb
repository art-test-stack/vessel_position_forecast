{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf999f41-a08c-45cc-b1ea-7e997c6ce1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8ef4f-78de-417a-ae79-9d08377cee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c338d-2dfd-4fca-88e3-a99163352f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea097c04-76b0-41a5-95b0-9aaef6fee28d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023318e-40e7-4d7c-8e50-2bd4ad53e7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_data(train_data_path, test_data_path, delimiter=\"|\"):\n",
    "    # Load the training and test data\n",
    "    train_data = pd.read_csv(train_data_path, delimiter=delimiter)\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    test_data = test_data.drop(columns=[\"ID\", \"scaling_factor\"])\n",
    "\n",
    "    # Convert time column to datetime\n",
    "    train_data['time'] = pd.to_datetime(train_data['time'])\n",
    "\n",
    "    # Sort the training data by vesselId and time to ensure proper alignment for shifting\n",
    "    train_data_sorted = train_data.sort_values(by=[\"vesselId\", \"time\"], ascending=True).copy()\n",
    "\n",
    "    # Shift latitude and longitude for each vessel to create \"previous_lat\" and \"previous_lon\"\n",
    "    train_data_sorted['previous_lat'] = train_data_sorted.groupby('vesselId')['latitude'].shift(1)\n",
    "    train_data_sorted['previous_lon'] = train_data_sorted.groupby('vesselId')['longitude'].shift(1)\n",
    "\n",
    "    # Get the last entry per vessel in the training data\n",
    "    last_entries = train_data_sorted.groupby('vesselId').last().reset_index()\n",
    "    \n",
    "    # Merge the last row of the training data with the test data based on vesselId\n",
    "    test_data_merged = test_data.merge(last_entries, on='vesselId', how='left')\n",
    "    test_data_merged = test_data_merged.drop(columns=['time_y']).rename(columns={'time_x': 'time'})\n",
    "\n",
    "    return train_data_sorted, test_data_merged\n",
    "\n",
    "# Usage\n",
    "train_data_path = AIS_TRAIN # '../data/ais_train.csv'\n",
    "test_data_path = AIS_TEST # '../data/ais_test.csv'\n",
    "\n",
    "processed_train_data, processed_test_data = process_data(train_data_path, test_data_path)\n",
    "\n",
    "# To save the processed test data\n",
    "processed_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e54ab-d889-48b6-a244-e2e087242d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_test = processed_test_data\n",
    "X_test = X_test.drop(columns=[\"longitude\", \"latitude\", \"etaRaw\"])\n",
    "\n",
    "# Prepare features and target variables\n",
    "y_train = processed_train_data[[\"latitude\", \"longitude\"]].copy()\n",
    "X_train = processed_train_data.copy()\n",
    "X_train = X_train.drop(columns=[\"longitude\", \"latitude\", \"etaRaw\", \"navstat\", \"rot\", \"heading\"])\n",
    "\n",
    "# Basic preprocessing (converting time to numerical features)\n",
    "X_train['time'] = pd.to_datetime(X_train['time'], errors='coerce')\n",
    "X_train['year'] = X_train['time'].dt.year\n",
    "X_train['month'] = X_train['time'].dt.month\n",
    "X_train['day'] = X_train['time'].dt.day\n",
    "X_train['hour'] = X_train['time'].dt.hour\n",
    "X_train = X_train.drop(columns=['time'])\n",
    "\n",
    "# Apply the same preprocessing to the test set\n",
    "X_test = processed_test_data.copy()\n",
    "X_test = X_test.drop(columns=[\"longitude\", \"latitude\", \"etaRaw\", \"navstat\", \"rot\", \"heading\"])\n",
    "\n",
    "X_test['time'] = pd.to_datetime(X_test['time'], errors='coerce')\n",
    "X_test['year'] = X_test['time'].dt.year\n",
    "X_test['month'] = X_test['time'].dt.month\n",
    "X_test['day'] = X_test['time'].dt.day\n",
    "X_test['hour'] = X_test['time'].dt.hour\n",
    "X_test = X_test.drop(columns=['time'])\n",
    "\n",
    "# Encode categorical variables 'vesselId' and 'portId' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "X_train['vesselId'] = label_encoder.fit_transform(X_train['vesselId'].astype(str))\n",
    "X_test['vesselId'] = label_encoder.transform(X_test['vesselId'].astype(str))  # Use transform to avoid data leakage\n",
    "\n",
    "X_train['portId'] = label_encoder.fit_transform(X_train['portId'].astype(str))\n",
    "X_test['portId'] = label_encoder.transform(X_test['portId'].astype(str))  # Use transform to avoid data leakage\n",
    "\n",
    "# Select relevant features for training and testing\n",
    "X_train = X_train[['cog', 'sog', 'previous_lat', 'previous_lon', 'vesselId', 'portId']]\n",
    "X_test = X_test[['cog', 'sog', 'previous_lat', 'previous_lon', 'vesselId', 'portId']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd3e38-f65d-4429-bf42-4f3e36fa7743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xreg = xgb.XGBRegressor()\n",
    "\n",
    "# Wrap the model in MultiOutputRegressor for multi-target regression\n",
    "multi_regressor = MultiOutputRegressor(xreg)\n",
    "\n",
    "#xreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe701db-2668-4193-89f3-098b09309f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multi_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf74df-ce8a-4c1e-a8b2-6a3b36c9dec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#xgb.plot_importance(booster=xreg ); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08464469-b61a-43b3-8d29-fb4bfb4e19db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'estimator__n_estimators': [100, 500],  # Number of boosting rounds\n",
    "    'estimator__max_depth': [3, 6],        # Maximum tree depth\n",
    "    'estimator__learning_rate': [0.01, 0.1],  # Learning rate (eta)\n",
    "    'estimator__subsample': [0.8, 1.0],     # Subsample ratio\n",
    "    'estimator__min_child_weight': [1, 5],  # Minimum sum of instance weight needed in a child\n",
    "    'estimator__colsample_bytree': [0.8, 1.0]  # Subsample ratio of columns when constructing trees\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=multi_regressor,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # Cross-validation folds\n",
    "    verbose=10,\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cb8de4-6cf9-4068-a7ce-e8a55a3a44c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699f932-7764-4c2d-8c15-03c2c0275974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf5d89-ce2f-4078-9f18-2f09c8ca8938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params = {'estimator__colsample_bytree': 0.8, \n",
    " 'estimator__learning_rate': 0.1, \n",
    " 'estimator__max_depth': 6,\n",
    " 'estimator__min_child_weight': 1,\n",
    " 'estimator__n_estimators': 100,\n",
    " 'estimator__subsample': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74073a6-264e-4d93-a6ae-2f365b288ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions over X_test\n",
    "y_pred_default = grid_search.predict(X_test)\n",
    "\n",
    "# Convert the NumPy array to a pandas DataFrame\n",
    "y_pred_default_df = pd.DataFrame(y_pred_default, columns=['latitude_predicted', 'longitude_predicted'])\n",
    "\n",
    "# Add the ID column, which starts from 0 and increments by 1 for each row\n",
    "y_pred_default_df['ID'] = range(len(y_pred_default_df))\n",
    "\n",
    "# Reorder the columns to ensure 'id' is the first column (if required by the submission format)\n",
    "y_pred_default_df = y_pred_default_df[['ID', 'longitude_predicted', 'latitude_predicted']]\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "from utils import make_file_name\n",
    "from settings import * \n",
    "\n",
    "output_path = SUBMISSION_FODLER.joinpath(make_file_name() + '.csv')\n",
    "y_pred_default_df.to_csv(output_path, index=False)\n",
    "\n",
    "y_pred_default_df.head()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu118.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu118:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
